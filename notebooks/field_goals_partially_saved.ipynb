{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "pymc3_kernel",
   "display_name": "pymc3_kernel"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In 2013, in the paper Going For Three, the authors undertook predictive modelling of field goal success in the NFL from 2000 to 2011. The study found that several psychological factors had little to no effect on the likelihood of field goal conversion. In 2018, in the paper Choking Under the Pressure, the authors extended the aforementioned study to include data post-2011, up to and including 2017. They found that some of the psychological factors were in fact significant at the 0.1 level.\n",
    "\n",
    "Neither study looked into the possibility of interactions between the supposedly (independent) variables, which may dilute the observed main effects or even show the effect to be significant. For example, one may reasonably expect that the effect of wind on a kick would increase with distance. Futhermore, in 2015, a rule change saw the extra point distance increased from 20 yards to 33. 20 yard extra point kicks historically average around 99% conversion. The 13 yard increase resulted in a 6% point drop the year of its inception, which continues to this day. As such, we will model only field goals up until 2015, at which time we will then include extra point kicks.\n",
    "\n",
    "Both studies took a frequentist statistical approach, which although provide frequency guarantees with regards to confidence intervals etc, accept and reject significance of variables at arbitrarly decided values of 0.05 and 0.1. Within Bayesian statistics, by framing parameters as random variables, we are able to give likelihoods of each variable being within some range (credible interval). Arguably more intuitive than the frequentist confidence interval. For small sample sizes, we do forgo some of the frequentist guarantees of confidence intervals, but these concerns disappear for larger samples as the two methods converge asymptotically. With this in mind, we will be using a frequentist approach until the 2015 rule change. The results of this period of time will be used as priors to a Bayesian statistical model which will handle post-rule change data.\n",
    "\n",
    "This notebook will include exploring the data, validating the results of the earlier papers, and exploring the possibility of interactions. With the most influential interactions, we will use a frequentist approach to the pre-2015 data, and see how our results differ from Going For Three. In the next notebook, we will incorporate these results into the prior distributions for the independent variables and train a model on post-2015 data using MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm as glm_sm\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import itertools\n",
    "import dtale\n",
    "from scipy.stats import chi2, pointbiserialr, pearsonr\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util.metrics import tetrachoric\n",
    "from util.data import clean, get_data\n",
    "from util.summary import Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set up a connection to the mysql database served locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = mysql.connector.connect(user='root', password='mOntie20!mysql', host='127.0.0.1', database='nfl')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We shall first load all the data and inspect it with the d-tale tool or similar pandas functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select g.seas, avg(f.good) from fgxp f join play p on f.pid=p.pid join game g on g.gid=p.gid where f.fgxp='xp' group by g.seas'''\n",
    "df_xp = pd.read_sql(query,cnx)\n",
    "df_xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where = '''\\nand (\n",
    "(\n",
    "    fg.fkicker in (select fkicker from fifty) -- has had at least 50 attempts overall (this keeps only kickers that would end up making it in the NFL)\n",
    ") or    \n",
    "(\n",
    "    k.seas>=3  -- or they had played 3 seasons up to the kick (stops removal of kicks from experienced kickers' kicks from early or late in the dataset)\n",
    ")\n",
    ")'''\n",
    "\n",
    "df = get_data(cnx, '< 2015', where=where, base='raw_6_cat')\n",
    "print(df.columns.values)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fkicker</th>\n      <th>good</th>\n      <th>dist</th>\n      <th>year</th>\n      <th>seasons</th>\n      <th>temperature</th>\n      <th>home_team</th>\n      <th>stadium</th>\n      <th>FG</th>\n      <th>age</th>\n      <th>...</th>\n      <th>humid</th>\n      <th>wind</th>\n      <th>away_game</th>\n      <th>postseason</th>\n      <th>iced</th>\n      <th>turf</th>\n      <th>precipitation</th>\n      <th>pressure</th>\n      <th>form</th>\n      <th>kicks</th>\n    </tr>\n    <tr>\n      <th>pid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>43</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>44</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>24</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>44</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>48</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>",
      "text/plain": "     fkicker  good  dist  year  seasons  temperature home_team       stadium  \\\npid                                                                            \n17   MA-0700     1    43  2000       19           79       ATL  Georgia Dome   \n34   MA-0700     1    44  2000       19           79       ATL  Georgia Dome   \n52   MA-0700     1    24  2000       19           79       ATL  Georgia Dome   \n64   MA-0700     1    44  2000       19           79       ATL  Georgia Dome   \n95   MA-0700     1    48  2000       19           79       ATL  Georgia Dome   \n\n     FG  age  ...  humid  wind  away_game  postseason  iced  turf  \\\npid           ...                                                   \n17    1   40  ...      0     0          0           0     0     1   \n34    1   40  ...      0     0          0           0     0     1   \n52    1   40  ...      0     0          0           0     0     1   \n64    1   40  ...      0     0          0           0     0     1   \n95    1   40  ...      0     0          0           0     0     1   \n\n     precipitation  pressure  form  kicks  \npid                                        \n17               0         1   NaN      0  \n34               0         1   1.0      0  \n52               0         1   1.0      0  \n64               0         1   1.0      0  \n95               0         1   1.0      0  \n\n[5 rows x 21 columns]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean(df)\n",
    "# df.drop('FG', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "fkicker    \nAD-0800  0      0\n         1      1\n         2      2\n         3      3\n         4      4\n               ..\nWR-0500  50    50\n         51    51\n         52    52\n         53    53\n         54    54\nLength: 14299, dtype: int64"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('fkicker').apply(lambda x: pd.Series(list(range(len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fkicker</th>\n      <th>good</th>\n      <th>dist</th>\n      <th>year</th>\n      <th>seasons</th>\n      <th>temperature</th>\n      <th>home_team</th>\n      <th>stadium</th>\n      <th>FG</th>\n      <th>age</th>\n      <th>...</th>\n      <th>humid</th>\n      <th>wind</th>\n      <th>away_game</th>\n      <th>postseason</th>\n      <th>iced</th>\n      <th>turf</th>\n      <th>precipitation</th>\n      <th>pressure</th>\n      <th>form</th>\n      <th>kicks</th>\n    </tr>\n    <tr>\n      <th>pid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>43</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>44</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>24</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>44</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>MA-0700</td>\n      <td>1</td>\n      <td>48</td>\n      <td>2000</td>\n      <td>19</td>\n      <td>79</td>\n      <td>ATL</td>\n      <td>Georgia Dome</td>\n      <td>1</td>\n      <td>40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>",
      "text/plain": "     fkicker  good  dist  year  seasons  temperature home_team       stadium  \\\npid                                                                            \n17   MA-0700     1    43  2000       19           79       ATL  Georgia Dome   \n34   MA-0700     1    44  2000       19           79       ATL  Georgia Dome   \n52   MA-0700     1    24  2000       19           79       ATL  Georgia Dome   \n64   MA-0700     1    44  2000       19           79       ATL  Georgia Dome   \n95   MA-0700     1    48  2000       19           79       ATL  Georgia Dome   \n\n     FG  age  ...  humid  wind  away_game  postseason  iced  turf  \\\npid           ...                                                   \n17    1   40  ...      0     0          0           0     0     1   \n34    1   40  ...      0     0          0           0     0     1   \n52    1   40  ...      0     0          0           0     0     1   \n64    1   40  ...      0     0          0           0     0     1   \n95    1   40  ...      0     0          0           0     0     1   \n\n     precipitation  pressure  form  kicks  \npid                                        \n17               0         1   NaN      0  \n34               0         1   1.0      0  \n52               0         1   1.0      0  \n64               0         1   1.0      0  \n95               0         1   1.0      0  \n\n[5 rows x 21 columns]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations & Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We assess the correlations: continuous pairs with Pearsons, continuous-binary with pointbiserial, binary pairs with tetrachoric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "division by zero\nfkicker-FG: NaN\ndivision by zero\nfkicker-age: NaN\ndivision by zero\nfkicker-altitude: NaN\ndivision by zero\nfkicker-away_game: NaN\ndivision by zero\ngood-fkicker: NaN\ndivision by zero\ngood-FG: NaN\ndivision by zero\ngood-age: NaN\ndivision by zero\nhome_team-fkicker: NaN\ndivision by zero\nhome_team-good: NaN\ndivision by zero\nhome_team-FG: NaN\ndivision by zero\nhome_team-age: NaN\ndivision by zero\nhome_team-altitude: NaN\ndivision by zero\nhome_team-away_game: NaN\ndivision by zero\nhome_team-form: NaN\ndivision by zero\nstadium-fkicker: NaN\ndivision by zero\nstadium-good: NaN\ndivision by zero\nstadium-home_team: NaN\ndivision by zero"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-38abebd77081>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mcorrelations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtetrachoric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tetrachoric'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lewis.Bails\\Repositories\\nfl_study\\util\\metrics.py\u001b[0m in \u001b[0;36mtetrachoric\u001b[1;34m(c1, c2)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc10\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mc21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m180\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-38abebd77081>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mcorrelations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtetrachoric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tetrachoric'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{v1}-{v2}: NaN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\conda_pymc3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\conda_pymc3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\conda_pymc3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\conda_pymc3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "con_vars = ['dist','year','seasons','temperature','wind','pressure']\n",
    "bin_vars = df.drop(con_vars, axis=1).columns.values\n",
    "\n",
    "correlations = []\n",
    "\n",
    "for v1, v2 in itertools.product(bin_vars,bin_vars):\n",
    "    if v1>v2:\n",
    "        try:\n",
    "            correlations.append([v1,v2,tetrachoric(df[v1],df[v2]),None,'tetrachoric'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'{v1}-{v2}: NaN')\n",
    "\n",
    "for b1, c2 in itertools.product(bin_vars,con_vars):\n",
    "    try:\n",
    "        correlations.append([b1, c2, pointbiserialr(df[b1],df[c2])[0], pointbiserialr(df[b1],df[c2])[1], 'pointbiserial'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'{b1}-{c2}: {e}')\n",
    "\n",
    "for c1, c2 in itertools.product(con_vars,con_vars):\n",
    "    if c1>c2:\n",
    "        try:\n",
    "            correlations.append([c1, c2, pearsonr(df[c1],df[c2])[0], pearsonr(df[c1],df[c2])[1], 'pearsons'])\n",
    "        except Exception as e:\n",
    "           print(f'{c1}-{c2}: {e}')\n",
    "\n",
    "\n",
    "df_corr = pd.DataFrame.from_records(correlations, columns=['cov 1','cov 2', 'corr', 'p','type'])\n",
    "df_corr[abs(df_corr['corr'])>0.2].sort_values('corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "From the table above we can see a few things.\n",
    "* The strongest correlations belong to the binary variables\n",
    "* turf and altitude have a strong correlation because there is only one stadium at altitude, Denver, and that stadium has a grass surface.\n",
    "* Weather conditions are relatively strongly correlated to altitude. Which is a well known in meteorology.\n",
    "* Precipitation increases humidity, that should be obvious.\n",
    "* Coaches tend to ice kickers when the game is tight, i.e. pressure situations.\n",
    "* Age and seasons are strongly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.loc[:,con_vars], hue='good')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not much we can pull from the above scatters, but lets make a few points.\n",
    "* It looks like as kickers get late into their careers, they take fewer long distance attempts.\n",
    "* Distance of attempts for each year is relatively stable. Looks like 2008 has a few massively long attempts.\n",
    "* Seasons only increasings year-on-year because the span isnt long enough to record retirements. So moving on, we'd expect less correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we bin the continuous variables to make the plots readable.\n",
    "df_ = df.copy()\n",
    "df['seas_bin'] = 4*round(df['seasons']/4)\n",
    "df['temp_bin'] = 4*round(df['temperature']/4)\n",
    "df['age_bin'] = 4*round(df['age']/4)\n",
    "df['dist_bin'] = 4*round(df['dist']/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [('seas_bin',y,'away_game') for y in bin_vars if y not in ('away_game','good')]\n",
    "kwargs = lambda z: {'x':z[0], 'y':z[1], 'hue':z[2], 'data':df_}\n",
    "for pair in pairs:\n",
    "    sns.lineplot(**kwargs(pair))\n",
    "    plt.title(pair[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [('seas_bin', 'good', None),('dist_bin', 'good', None)]\n",
    "for pair in pairs:\n",
    "    sns.lineplot(**kwargs(pair))\n",
    "    plt.title(pair[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base result\n",
    "formula = 'good ~ ' + '+'.join(df.drop(['good'], axis=1).columns.values)\n",
    "model = glm_sm(formula, df, family=sm.families.Binomial())\n",
    "result = model.fit(method='irls')\n",
    "with_ = Summary(result)\n",
    "print(with_)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neither of the aforementioned papers addressed the possibility of multicollinearity, omitted variable bias.\n",
    "\n",
    "In the LR section, we remove each of the variables one-by-one and assess the changes in expected values and standard errors of coefficients of the remaining variables. Multicollinearity affects the standard errors of correlated variables and reduces stability when present. However, if a variable is correlated to both the dependent and an independent variable but is omitted, the remaining correlated variable exhibits bias (omitted variable bias or OVB).\n",
    "\n",
    "In the LASSO section, we perform regularised regression on the normalized data and see which variables have non-zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood Ratio Test"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can tabulate the results, showing the change in expected coefficients and std errs when we include/exclude each covariate. A summary of the findings is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = {}\n",
    "for cov in df.drop(['good'], axis=1).columns.values:\n",
    "    formula = 'good ~ ' + '+'.join(df.drop(['good',cov], axis=1).columns.values)\n",
    "    model = glm_sm(formula, df, family=sm.families.Binomial())\n",
    "    without_ = Summary(model.fit(method='irls'))\n",
    "    difference, p = with_ - without_\n",
    "    removed[cov] = {'diff':difference, 'p': p, 'summary': without_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LR test p-values:\\n', [(cov,round(d['p'],4)) for cov,d in removed.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = 'year'\n",
    "print(f'Results for \"{cov}\"\\n', 'P(LR|Ho): ', removed[cov]['p'])\n",
    "removed[cov]['diff']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* dist: OVB on iced and also goes siginificant, OVB with pressure but not significant anyway, maybe OVB with away_game but never significant. LR test rejected null.\n",
    "* altitude: Sigificant on its own, no real OVB seen elsewhere, apart from humid which wasnt significant anyway. LR test rejected null.\n",
    "* away_game: No OVB and not siginificant to begin with. LR test did not reject null.\n",
    "* cold: OVB on postseason (which also became significant w/o cold). Slight OVB with windy. LR test rejected null.\n",
    "* humid: No change on any covariate estimates. LR test did not reject null.\n",
    "* iced: OVB on pressure but it never became significant. LR test did not reject null, however it was close.\n",
    "* postseason: Nothing. LR test did not reject null.\n",
    "* precipitation: OVB on humidity (theyre correlated), but no changes on significance. LR test rejected null.\n",
    "* pressure: No change. Not even an OVB on iced. LR test did not reject null.\n",
    "* seasons: No change. Is significant variable. LR test rejected null.\n",
    "* turf: OVB on humidity, turf pitches must be in warmer climates. LR test rejected null.\n",
    "* windy: No change. LR test rejected null.\n",
    "* year: No change. Slight OVB on seasons as we didnt see too many retirements for 2000-2015. LR test rejected null.\n",
    "\n",
    "Points:\n",
    "* cold vs postseason -- postseason is in winter so removing cold resulting in strong OVB on postseason estimate.\n",
    "* dist vs iced -- Dist is a strong indicator, and iced kicks are slightly longer on average, hence the OVB.\n",
    "* standard errors not largely different from control to treatment, but OVB seems significant for a few combinations.\n",
    "* Humidity adds nothing and we see no OVB with its omissions. We'll remove it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularisation enforces sparsity. So assuming our variables are on the same scale, the largest absolute coefficients should indicate the most important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.features import LASSO\n",
    "\n",
    "summary = LASSO(df, doubles=False, triples=False)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly distance is the biggest factor, followed by year, altitude, cold, seasons, surface and precipitation. Psychological factors like postseason, iced, pressure, and away_game are less important. Humidity, as was the case in the LR test, is the least important."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iteractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['humid','age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base result now without humid and age\n",
    "formula = 'good ~ ' + '+'.join(df.drop(['good'], axis=1).columns.values)\n",
    "model = glm_sm(formula, df, family=sm.families.Binomial())\n",
    "result = model.fit(method='irls')\n",
    "without_ = Summary(result)\n",
    "print(without_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO\n",
    "Performing LASSO is one way of finding the most important variables and their interactions. We'll create variables for all the interactions, then bootstrap several times and find the coefficients of the penalised LR model each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = df.drop(['good'], axis=1).columns.values\n",
    "summary = LASSO(df, covariates, triples=False, drop_post=['age'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood Ratio Test\n",
    "\n",
    "We expect to get a whole heap of false positives if we test all interactions. So we only test those from LASSO test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.feature import get_interactions\n",
    "\n",
    "covariates = ['iced', 'away_game', 'seasons', 'dist', 'temperature', 'precipitation']\n",
    "interactions = get_interactions(covariates)\n",
    "\n",
    "inters = {}\n",
    "for interaction in interactions:\n",
    "    formula = 'good ~ ' + '+'.join(df.drop(['good'], axis=1)) + '+' + interaction\n",
    "    model = glm_sm(formula, df, family=sm.families.Binomial())\n",
    "    with_ = Summary(model.fit(method='irls'))\n",
    "    diff, p = with_ - without_\n",
    "    inters[interaction] = [interaction, with_.log_likelihood - without_.log_likelihood, p, diff]\n",
    "\n",
    "df_inter = pd.DataFrame.from_records(list(inters.values()), columns=['interaction','delta_ll','p', 'diff']).set_index('interaction')\n",
    "df_inter[df_inter['p']<0.4].sort_values('p', ascending=True).loc[:,['delta_ll','p']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pressure\\*altitude shows up again. Seasons\\*dist is plausible. Windy\\*postseason seems like an interaction by proxy (with cold). Seasons\\*away_game shows up again. Iced*altitude is spurious. Windy\\*cold is plausible. Iced\\*away_game is plausible. Precipitation\\*dist is plausible. Iced\\*cold is plausible.\n",
    "\n",
    "Now we investigate graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again we bin continuous variables for clarity\n",
    "df_ = df.copy()\n",
    "df['seas_bin'] = 4*round(df['seasons']/4)\n",
    "df['temp_bin'] = 4*round(df['temperature']/4)\n",
    "df['dist_bin'] = 4*round(df['dist']/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = lambda z: {'x':z[0], 'y':z[1], 'hue':z[2], 'data':df_}\n",
    "pairs = [('iced', 'good', 'away_game'),\n",
    "         ('dist_bin', 'good', 'seas_bin'),\n",
    "         ('temp_bin', 'good', 'precipitation'),\n",
    "        ]\n",
    "\n",
    "for pair in pairs:\n",
    "    sns.lineplot(**kwargs(pair))\n",
    "    plt.title(pair[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('P(LR|Ho): ',df_inter.loc['iced*away_game', 'p'])\n",
    "df_inter.loc['iced*away_game', 'diff']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Points:\n",
    "* It's tough to separate the effect of postseason games and the cold. There's never a postseason game during the warmer months so maybe we'll have to do an observational study for this. My thoughts are its the cold doing the work, because the OVB when leaving out cold is significant on the postseason coefficient but not other way around. But pressure\\*postseason makes more sense than pressure\\*cold.\n",
    "\n",
    "* Choose: seasons\\*dist, windy\\*cold, iced\\*away_game, pressure\\*postseason, pressure\\*iced.\n",
    "\n",
    "* I wanted to keep wind\\*dist but there was no evidence that supported my hypothesis.\n",
    "\n",
    "\n",
    "From LASSO:\n",
    "* iced\\*away_game\n",
    "* windy\\*cold\n",
    "\n",
    "From LR:\n",
    "* seasons\\*dist\n",
    "* pressure\\*iced (not sig)\n",
    "* pressure\\*postseason (not sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = '+ seasons*dist + iced*away_game + temperature*precipitation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base result without interactions\n",
    "formula = 'good ~ ' + '+'.join(df.drop(['good'], axis=1).columns.values)\n",
    "model = glm_sm(formula, df, family=sm.families.Binomial())\n",
    "base_result = model.fit(method='irls')\n",
    "without_ = Summary(base_result)\n",
    "print(without_.result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result with the interactions\n",
    "formula = 'good ~ ' + '+'.join(df.drop(['good'], axis=1).columns.values) + interactions\n",
    "model = glm_sm(formula, df, family=sm.families.Binomial())\n",
    "result = model.fit(method='irls')\n",
    "with_ = Summary(result)\n",
    "diff, p = with_ - without_\n",
    "print(f'P(LR|Ho):{p}\\n', f'With interactions:{with_.log_likelihood}\\n', f'Without interactions:{without_.log_likelihood}')\n",
    "print(with_.result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = dt.now().strftime('%d%m%y')\n",
    "with_.summary.to_csv(f'fg_results_{now}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "# BROKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.evaluation import odds, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_coefs = with_.summary['coef'].drop(['windy','pressure','form'])\n",
    "\n",
    "confusion(['away_game', 'iced'], sig_coefs)\n",
    "confusion(['cold', 'windy'], sig_coefs)\n",
    "confusion(['seasons','dist'], sig_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odds(**{'seasons':15, 'dist':55})/odds(**{'seasons':0, 'dist':55})\n",
    "# odds(**{'seasons':15})/odds(**{'seasons':0})\n",
    "# odds(**{'away_game':0, 'iced':1, 'pressure':6})/odds(**{'away_game':0, 'iced':0, 'pressure':6})\n",
    "# # odds(**{'pressure':1, 'iced':1})/odds(**{'pressure':1, 'iced':0})\n",
    "# # odds(**{'pressure':6, 'iced':1})/odds(**{'pressure':6, 'iced':0})\n",
    "\n",
    "# odds(**{'seasons':1, 'dist':50})/odds(**{'seasons':1, 'dist':49})\n",
    "# odds(**{'away_game':1, 'iced':0, 'pressure':6})/odds(**{'away_game':0, 'iced':0, 'pressure':6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give some examples of influence.\n",
    "\n",
    "\n",
    "* Icing an away game kick increases the odds 1.07 times. Weird.\n",
    "* Icing a home kick, odds are 0.75 times.\n",
    "\n",
    "* Un-iced, an away kick, odds are 0.91 times.\n",
    "* Iced, an away kick, odds are \n",
    "\n",
    "\n",
    "\n",
    "* For a rookie, every yard added the odds are about 0.9 times.\n",
    "* However for a veteran of 15 seasons, its gets a bit complicated. Experience matters, but so does the body.\n",
    "* Over the rookie, at 35 yards (the average kick), the veteran is about 1.5 times more likely to make the kick. If we dont account for distance interacting with the kickers weary old age, this blows out to 3.1 times as likely to convert.\n",
    "* At 55 yards, a rookie would narrowly pip an experienced 15 season veteran, with the odds ratio dropping to 0.91. Again, not taking the interaction into consideration and \"father time\" blows out to 3.4 times as likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We'll use these values in the next study, which is a Bayesian approach to post-2015 data."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Even though we are interested in accurate inference, let's check the model isn't complete garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = df_test['good']\n",
    "\n",
    "# base\n",
    "y_p = base_result.predict(df_test.drop('good',axis=1))\n",
    "auc_base = roc_auc_score(y_t, y_p)\n",
    "curve_base = roc_curve(y_t, y_p)\n",
    "df_base = pd.DataFrame.from_records(np.column_stack(curve_base), columns=['fpr','tpr','thresholds'])\n",
    "df_base['type'] = 'base'\n",
    "\n",
    "# interactions\n",
    "y_p = result.predict(df_test.drop('good',axis=1))\n",
    "auc = roc_auc_score(y_t, y_p)\n",
    "curve = roc_curve(y_t, y_p)\n",
    "df_int = pd.DataFrame.from_records(np.column_stack(curve), columns=['fpr','tpr','thresholds'])\n",
    "df_int['type'] = 'interactions'\n",
    "\n",
    "# dummy - good\n",
    "y_p = [1]*len(df_test)\n",
    "y_p = pd.Series(y_p)\n",
    "auc_dg = roc_auc_score(y_t, y_p)\n",
    "curve_dg = roc_curve(y_t, y_p)\n",
    "df_dg = pd.DataFrame.from_records(np.column_stack(curve_dg), columns=['fpr','tpr','thresholds'])\n",
    "df_dg['type'] = 'All good'\n",
    "\n",
    "# dummy - random @ P(good)\n",
    "p = df_train['good'].value_counts(normalize=True)[1]\n",
    "y_p = pd.Series([1 if p>np.random.rand() else 0 for _ in range(len(df_test))])\n",
    "auc_dp = roc_auc_score(y_t, y_p)\n",
    "curve_dp = roc_curve(y_t, y_p)\n",
    "df_dp = pd.DataFrame.from_records(np.column_stack(curve_dp), columns=['fpr','tpr','thresholds'])\n",
    "df_dp['type'] = 'P(good)'\n",
    "\n",
    "# dummy - @ dist>40\n",
    "y_p = df_test['dist']>40\n",
    "auc_dd = roc_auc_score(y_t, y_p)\n",
    "curve_dd = roc_curve(y_t, y_p)\n",
    "df_dd = pd.DataFrame.from_records(np.column_stack(curve_dd), columns=['fpr','tpr','thresholds'])\n",
    "df_dd['type'] = '@d>40'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs = pd.Series([auc_dp, auc_dg, auc_base, auc], index=['@p','good','base','with interactions'])\n",
    "aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import lineplot\n",
    "df_all = pd.concat([df_base,df_int,df_dg,df_dp,df_dd])\n",
    "ax = lineplot(x='fpr',y='tpr', data=df_all, hue='type')\n",
    "ax.set_title('ROC for each model')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Interactions and base model have the same predictive power, but I'd argue the interactions provide more accuracte inference. Both are better than the simplest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See you in the next notebook."
   ]
  }
 ]
}